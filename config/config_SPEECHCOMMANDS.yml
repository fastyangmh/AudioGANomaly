# parameters configuration
mode: train
root: data/
predefined_dataset: SPEECHCOMMANDS
classes: ["normal", "abnormal"]
max_samples: null
batch_size: 32
num_workers: 0
device: cuda
sample_rate: 16000
lr: 1e-3
in_chans: 1
input_height: 64
latent_dim: 256
generator_feature_dim: 64
discriminator_feature_dim: 64
adversarial_weight: 1
reconstruction_weight: 50
encoding_weight: 1
checkpoint_path: null

# transforms configuration
transforms_config:
  train:
    PadWaveform:
      max_waveform_length: 16000
    MelSpectrogram:
      sample_rate: 16000
      n_mels: 64
    AmplitudeToDB: null
    Resize:
      - 64
      - 64

  val:
    PadWaveform:
      max_waveform_length: 16000
    MelSpectrogram:
      sample_rate: 16000
      n_mels: 64
    AmplitudeToDB: null
    Resize:
      - 64
      - 64

  test:
    PadWaveform:
      max_waveform_length: 16000
    MelSpectrogram:
      sample_rate: 16000
      n_mels: 64
    AmplitudeToDB: null
    Resize:
      - 64
      - 64

  predict:
    PadWaveform:
      max_waveform_length: 16000
    MelSpectrogram:
      sample_rate: 16000
      n_mels: 64
    AmplitudeToDB: null
    Resize:
      - 64
      - 64

# target transforms configuration
target_transforms_config:
  train: null

  val: null

  test: null

  predict: null

# optimizers configuration
optimizers_config:
  Adam:
    betas:
      - 0.9
      - 0.999
    eps: 1e-08
    weight_decay: 0
    amsgrad: False

# learning rate schedulers configuration
lr_schedulers_config:
  CosineAnnealingLR:
    T_max: 10
